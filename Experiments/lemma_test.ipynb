{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')  # Don't forget to download stopwords!\n",
    "\n",
    "def nlpsteps(text):\n",
    "\n",
    "    # Remove punctuation\n",
    "    removed_punctuation = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    removed_punctuation = removed_punctuation.lower()\n",
    "    tokens = removed_punctuation.split()\n",
    "\n",
    "    # print(tokens)\n",
    "    \n",
    "    # # Tokenize\n",
    "    # tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and 'not' is preserved\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in all_stopwords]\n",
    "    # print(\"filtered\", filtered_tokens)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in filtered_tokens]  # Use 'v' for verbs\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"not created cannot couldn't wouldnot notewothy is not good can't won't would not\"\n",
    "preprocessed_text = nlpsteps(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dc68875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid version ear project not_create create new application client project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def nlpsteps(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by handling negation, tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Handle negation and remove stopwords\n",
    "    processed_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] == 'not' and i + 1 < len(tokens) and tokens[i + 1] not in all_stopwords:\n",
    "            lemmatized_next_word = lemmatizer.lemmatize(tokens[i + 1], pos='v')\n",
    "            processed_tokens.append('not_' + lemmatized_next_word)\n",
    "            i += 2  # Skip the next word as it has been concatenated\n",
    "        else:\n",
    "            if tokens[i] not in all_stopwords:\n",
    "                lemmatized_word = lemmatizer.lemmatize(tokens[i], pos='v')\n",
    "                processed_tokens.append(lemmatized_word)\n",
    "            i += 1\n",
    "\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"Invalid version of EAR project not created when creating a new Application Client Project\"\n",
    "preprocessed_text = nlpsteps(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7bfe8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid version ear project not_created create new application client project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def nlpsteps(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by handling negation, tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Handle negation and remove stopwords\n",
    "    processed_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] == 'not' and i + 1 < len(tokens) and tokens[i + 1] not in all_stopwords:\n",
    "            processed_tokens.append('not_' + tokens[i + 1])\n",
    "            i += 2  # Skip the next word as it has been concatenated\n",
    "        else:\n",
    "            if tokens[i] not in all_stopwords:\n",
    "                processed_tokens.append(lemmatizer.lemmatize(tokens[i], pos='v'))\n",
    "            i += 1\n",
    "\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"Invalid version of EAR project not created when creating a new Application Client Project\"\n",
    "preprocessed_text = nlpsteps(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58221298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the preprocess function\n",
    "def nlpsteps(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Filter out stopwords and punctuation\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in all_stopwords and token not in string.punctuation]\n",
    "    \n",
    "    # Lemmatize the remaining tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower(), pos='v') for token in filtered_tokens]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"cannot could not wouldn't The cats are playing in the garden, they love to play, they played yesterday as well. They also play with another cat, wouldn't shouldn't wasn't cannot be more cute than this, noteworthy qwenotrew shautnot can't\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have the stopwords downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the default list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print the stopwords list\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79378ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"'re\": \" are\",\n",
    "    \"'s\": \" is\",\n",
    "    \"'d\": \" would\",\n",
    "    \"'ll\": \" will\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    # Use regular expressions to replace contractions\n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(contraction)), expansion, text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text, contractions)\n",
    "    \n",
    "    # Remove non-alphabetic characters except spaces\n",
    "    text = re.sub('[^a-zA-Z ]', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Filter out punctuation tokens and stopwords, and lemmatize the remaining tokens\n",
    "    filtered_tokens = [\n",
    "        lemmatizer.lemmatize(token.lower(), pos='v')\n",
    "        for token in tokens \n",
    "        if token.lower() not in all_stopwords and token not in string.punctuation\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"The cats are playing in the garden, they love to play, they played yesterday as well. They also play with another cat, wouldn't shouldn't wasn't cannot be more cute than this, noteworthy qwenotrew shautnot can't\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"'re\": \" are\",\n",
    "    \"'s\": \" is\",\n",
    "    \"'d\": \" would\",\n",
    "    \"'ll\": \" will\",\n",
    "    \"'t\": \" not\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    # Use regular expressions to replace contractions\n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(contraction)), expansion, text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by expanding contractions, handling negation, tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text, contractions)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    print(\"Tokens after word_tokenize:\", tokens)\n",
    "    \n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Handle negation and remove stopwords\n",
    "    processed_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] == 'not' and i + 1 < len(tokens) and tokens[i + 1] not in all_stopwords:\n",
    "            processed_tokens.append('not_' + tokens[i + 1])\n",
    "            i += 2  # Skip the next word as it has been concatenated\n",
    "        else:\n",
    "            if tokens[i].lower() not in all_stopwords:\n",
    "                processed_tokens.append(lemmatizer.lemmatize(tokens[i].lower(), pos='v'))\n",
    "            i += 1\n",
    "\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"cannot noteworthy is not good can't won't compile compiles compiling cats. The company does not have license\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')  # Don't forget to download stopwords!\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation (while preserving \"cannot\" intact)\n",
    "    removed_punctuation = re.sub(r\"[^\\w\\s]\", \" \", text)  # Removes punctuation but leaves words intact\n",
    "    removed_punctuation = removed_punctuation.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(removed_punctuation)\n",
    "\n",
    "    # Remove stopwords (keep \"not\")\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.discard('not')  # Ensures \"not\" is preserved\n",
    "    filtered_tokens = [token for token in tokens if token not in all_stopwords]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in filtered_tokens]  # Use 'v' for verbs\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"The cats are playing in the garden, they love to play, they played yesterday as well also plays with another cat,, wouldn't shouldn't me wasn't,cannot be more cute than this, noteworthy qwenotrew shautnot can't!\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508cebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"'re\": \" are\",\n",
    "    \"'s\": \" is\",\n",
    "    \"'d\": \" would\",\n",
    "    \"'ll\": \" will\",\n",
    "    \"'t\": \" not\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    # Use regular expressions to replace contractions\n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(contraction)), expansion, text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text, contractions)\n",
    "    \n",
    "    # Remove non-alphabetic characters except spaces\n",
    "    text = re.sub('[^a-zA-Z ]', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Filter out punctuation tokens and stopwords, and lemmatize the remaining tokens\n",
    "    filtered_tokens = [\n",
    "        lemmatizer.lemmatize(token.lower(), pos='v')\n",
    "        for token in tokens \n",
    "        if token.lower() not in all_stopwords and token not in string.punctuation\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"The cats are playing in the garden, they love to play, they played yesterday as well. They also play with another cat,, wouldn't shouldn't me wasn't,cannot be more cute than this, noteworthy qwenotrew shautnot can't\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
