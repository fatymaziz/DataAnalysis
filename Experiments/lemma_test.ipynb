{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9a9428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot notewothy not good compile compile compile compnay not licence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')  # Don't forget to download stopwords!\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Remove punctuation\n",
    "    removed_punctuation = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    removed_punctuation = removed_punctuation.lower()\n",
    "    tokens = removed_punctuation.split()\n",
    "\n",
    "    # print(tokens)\n",
    "    \n",
    "    # # Tokenize\n",
    "    # tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and 'not' is preserved\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in all_stopwords]\n",
    "    # print(\"filtered\", filtered_tokens)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in filtered_tokens]  # Use 'v' for verbs\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"cannot notewothy is not good can't won't compile compiles compiling. compnay does not have licence \"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7bfe8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot noteworthy not_good compile compile compile cat company not license\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by handling negation, tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Handle negation and remove stopwords\n",
    "    processed_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] == 'not' and i + 1 < len(tokens) and tokens[i + 1] not in all_stopwords:\n",
    "            processed_tokens.append('not_' + tokens[i + 1])\n",
    "            i += 2  # Skip the next word as it has been concatenated\n",
    "        else:\n",
    "            if tokens[i] not in all_stopwords:\n",
    "                processed_tokens.append(lemmatizer.lemmatize(tokens[i], pos='v'))\n",
    "            i += 1\n",
    "\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"cannot noteworthy is not good can't won't compile compiles compiling cats. The company does not have license\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "58221298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat play garden love play play yesterday well also play another cat would n't n't n't not cute noteworthy qwenotrew shautnot ca n't\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the preprocess function\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Filter out stopwords and punctuation\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in all_stopwords and token not in string.punctuation]\n",
    "    \n",
    "    # Lemmatize the remaining tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower(), pos='v') for token in filtered_tokens]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"The cats are playing in the garden, they love to play, they played yesterday as well. They also play with another cat, wouldn't shouldn't wasn't cannot be more cute than this, noteworthy qwenotrew shautnot can't\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import string\n",
    "# import re\n",
    "\n",
    "# # Download necessary resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')  # Don't forget to download stopwords!\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Tokenize\n",
    "#     tokens = word_tokenize(text)\n",
    "    \n",
    "#     # Remove stopwords and 'not' is preserved\n",
    "#     all_stopwords = set(stopwords.words('english'))\n",
    "#     all_stopwords.remove('not')\n",
    "    \n",
    "#     # Initialize WordNetLemmatizer\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "#     # Filter out punctuation tokens and stopwords, and lemmatize the remaining tokens\n",
    "#     filtered_tokens = [\n",
    "#         lemmatizer.lemmatize(token.lower(), pos='v') \n",
    "#         for token in tokens \n",
    "#         if token.lower() not in all_stopwords and token not in string.punctuation\n",
    "#     ]\n",
    "    \n",
    "#     return ' '.join(filtered_tokens)\n",
    "\n",
    "# # Example usage\n",
    "# text = \"The cats are playing in the garden, they love to play, they played yesterday as well also plays with another cat,, wouldn't shouldn't me wasn't,cannot be more cute than this, noteworthy qwenotrew shautnot can't\"\n",
    "# preprocessed_text = preprocess_text(text)\n",
    "# print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b960bce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'through', 'few', 'a', \"couldn't\", 'our', \"you're\", \"it's\", 'its', 'while', 'very', 'wouldn', 'until', 'at', 'these', \"hadn't\", 'most', 'was', 'don', \"haven't\", 'he', 'up', 'd', 'more', \"don't\", 'who', 'y', 'shouldn', 'being', 'below', 'mustn', 's', 'themselves', 'with', 'doing', 'which', 'doesn', 'will', 'having', \"mightn't\", 'hasn', 'that', 'not', 'here', 'once', 'my', 'during', 't', 'off', 'be', 'him', \"won't\", 'we', 'does', 'this', 'if', 'from', 'are', 'when', 'in', 'both', \"you've\", 'me', 'is', 'or', 'what', 'their', 'hadn', \"weren't\", 'itself', 'm', 'should', 'only', \"isn't\", 'it', 'did', 'they', 'been', 'the', 'nor', 'your', 'wasn', 'ma', 'no', \"shouldn't\", 'were', 'won', 'there', \"you'll\", 'some', \"doesn't\", \"should've\", 'of', 'mightn', 'i', \"wouldn't\", 'into', 'ain', 'for', 'other', \"wasn't\", 'after', 'hers', 'any', 'yours', 'same', 'didn', 'under', 'where', 'had', 'isn', 'over', 'do', 'against', 'than', 'needn', 'to', 'yourselves', 'by', 'shan', 'ourselves', 'yourself', 'can', \"mustn't\", 'aren', 'theirs', 'on', 'his', 'whom', 'am', 'before', \"you'd\", 'have', 'herself', 'own', 'again', \"hasn't\", \"aren't\", \"didn't\", 'o', \"needn't\", 'further', 'down', 've', 'above', 'haven', 'each', 'myself', 'couldn', 'so', 'she', 'such', 'ours', 'as', 'because', 'but', 'too', 'just', 're', 'her', 'now', 'himself', \"she's\", 'and', 'then', 'll', 'out', 'an', 'how', 'why', 'all', 'weren', 'them', \"shan't\", 'those', \"that'll\", 'you', 'between', 'has', 'about'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have the stopwords downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the default list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print the stopwords list\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "79378ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat play garden love play play yesterday well also play another cat would not not not cute noteworthy qwenotrew shautnot not\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"'re\": \" are\",\n",
    "    \"'s\": \" is\",\n",
    "    \"'d\": \" would\",\n",
    "    \"'ll\": \" will\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    # Use regular expressions to replace contractions\n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(contraction)), expansion, text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text, contractions)\n",
    "    \n",
    "    # Remove non-alphabetic characters except spaces\n",
    "    text = re.sub('[^a-zA-Z ]', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Filter out punctuation tokens and stopwords, and lemmatize the remaining tokens\n",
    "    filtered_tokens = [\n",
    "        lemmatizer.lemmatize(token.lower(), pos='v')\n",
    "        for token in tokens \n",
    "        if token.lower() not in all_stopwords and token not in string.punctuation\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"The cats are playing in the garden, they love to play, they played yesterday as well. They also play with another cat, wouldn't shouldn't wasn't cannot be more cute than this, noteworthy qwenotrew shautnot can't\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82bc1f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after word_tokenize: ['can', 'not', 'noteworthy', 'is', 'not', 'good', 'can', 'not', 'will', 'not', 'compile', 'compiles', 'compiling', 'cats', '.', 'The', 'company', 'does', 'not', 'have', 'license']\n",
      "not_noteworthy not_good not not_compile compile compile cat . company not license\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"'re\": \" are\",\n",
    "    \"'s\": \" is\",\n",
    "    \"'d\": \" would\",\n",
    "    \"'ll\": \" will\",\n",
    "    \"'t\": \" not\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    # Use regular expressions to replace contractions\n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(contraction)), expansion, text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by expanding contractions, handling negation, tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text, contractions)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    print(\"Tokens after word_tokenize:\", tokens)\n",
    "    \n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Handle negation and remove stopwords\n",
    "    processed_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] == 'not' and i + 1 < len(tokens) and tokens[i + 1] not in all_stopwords:\n",
    "            processed_tokens.append('not_' + tokens[i + 1])\n",
    "            i += 2  # Skip the next word as it has been concatenated\n",
    "        else:\n",
    "            if tokens[i].lower() not in all_stopwords:\n",
    "                processed_tokens.append(lemmatizer.lemmatize(tokens[i].lower(), pos='v'))\n",
    "            i += 1\n",
    "\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"cannot noteworthy is not good can't won't compile compiles compiling cats. The company does not have license\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')  # Don't forget to download stopwords!\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation (while preserving \"cannot\" intact)\n",
    "    removed_punctuation = re.sub(r\"[^\\w\\s]\", \" \", text)  # Removes punctuation but leaves words intact\n",
    "    removed_punctuation = removed_punctuation.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(removed_punctuation)\n",
    "\n",
    "    # Remove stopwords (keep \"not\")\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.discard('not')  # Ensures \"not\" is preserved\n",
    "    filtered_tokens = [token for token in tokens if token not in all_stopwords]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in filtered_tokens]  # Use 'v' for verbs\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"The cats are playing in the garden, they love to play, they played yesterday as well also plays with another cat,, wouldn't shouldn't me wasn't,cannot be more cute than this, noteworthy qwenotrew shautnot can't!\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd3eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508cebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"'re\": \" are\",\n",
    "    \"'s\": \" is\",\n",
    "    \"'d\": \" would\",\n",
    "    \"'ll\": \" will\",\n",
    "    \"'t\": \" not\",\n",
    "    \"'ve\": \" have\",\n",
    "    \"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    # Use regular expressions to replace contractions\n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(contraction)), expansion, text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text, contractions)\n",
    "    \n",
    "    # Remove non-alphabetic characters except spaces\n",
    "    text = re.sub('[^a-zA-Z ]', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Filter out punctuation tokens and stopwords, and lemmatize the remaining tokens\n",
    "    filtered_tokens = [\n",
    "        lemmatizer.lemmatize(token.lower(), pos='v')\n",
    "        for token in tokens \n",
    "        if token.lower() not in all_stopwords and token not in string.punctuation\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"The cats are playing in the garden, they love to play, they played yesterday as well. They also play with another cat,, wouldn't shouldn't me wasn't,cannot be more cute than this, noteworthy qwenotrew shautnot can't\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ba329-1711-4993-817d-da85b0065f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlpsteps(x):\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses a summary of a bug.\n",
    "\n",
    "    Args:\n",
    "        x (str): The summary text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after removing non-alphabetic characters, converting to lowercase,\n",
    "             lemmatizing words, and removing stopwords.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(x))\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Concatenate 'not' with the next word\n",
    "    processed_review = []\n",
    "    i = 0\n",
    "    while i < len(review):\n",
    "        if review[i] == 'not' and i + 1 < len(review):\n",
    "            processed_review.append('not' + review[i + 1])\n",
    "            i += 2  \n",
    "        else:\n",
    "            if review[i] not in all_stopwords:\n",
    "                processed_review.append(lemmatizer.lemmatize(review[i]))\n",
    "            i += 1\n",
    "\n",
    "    # Join the processed words back into a sentence\n",
    "    review = ' '.join(processed_review)\n",
    "    # print(\"review\",review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ac079-206f-486e-ac9c-3c4aea2aac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text to test the function\n",
    "text = \"I want to go for a run. running is good for health. i ran yesterday aswell\"\n",
    "\n",
    "# Apply the nlpsteps function\n",
    "processed_text = nlpsteps(text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure you have these resources downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Convert POS tag to format recognized by WordNetLemmatizer\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by handling contractions, removing stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove non-alphabetic characters except spaces (keep original text intact)\n",
    "    text = re.sub('[^a-zA-Z ]', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # POS tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove stopwords and lemmatize words with POS tagging\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    processed_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags if word not in all_stopwords]\n",
    "\n",
    "    # Join the processed words back into a sentence\n",
    "    processed_text = ' '.join(processed_tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Example usage\n",
    "text = \"cannot create Java EE artifacts servlet bean etc\"\n",
    "processed_text = preprocess_text(text)\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8755bd2-ce44-4215-8542-3b7007fc4300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b555f-0c84-418c-a9d9-6f2c81853997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7630863-6347-494a-a1e1-62244d243b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
