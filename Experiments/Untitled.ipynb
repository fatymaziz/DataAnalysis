{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29341bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming get_r1 and get_r2 functions are defined elsewhere\n",
    "def get_r1(ns, s):\n",
    "    return s / (s + ns) if (s + ns) > 0 else 0\n",
    "\n",
    "def get_r2(ns, s):\n",
    "    return ns / (s + ns) if (s + ns) > 0 else 0\n",
    "\n",
    "def lexicon_preprocess(severe_word_counts, nonsevere_word_counts):\n",
    "    \"\"\"\n",
    "    Calculates the ratios for Severe and NonSevere categories.\n",
    "\n",
    "    Args:\n",
    "        severe_word_counts: Dictionary containing word counts for Severe category.\n",
    "        nonsevere_word_counts: Dictionary containing word counts for NonSevere category.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Contains words with their counts and ratios for Severe and NonSevere categories.\n",
    "    \"\"\"\n",
    "    # Combine word counts into a single dictionary\n",
    "    all_words = set(severe_word_counts.keys()).union(set(nonsevere_word_counts.keys()))\n",
    "    all_data = {word: {'Severe': severe_word_counts.get(word, 0), 'NonSevere': nonsevere_word_counts.get(word, 0)} for word in all_words}\n",
    "\n",
    "    # Calculate ratios and prepare payload\n",
    "    payload_train = {}\n",
    "    for word, counts in all_data.items():\n",
    "        ns = counts.get('NonSevere', 0)\n",
    "        s = counts.get('Severe', 0)\n",
    "        r1 = get_r1(ns, s)\n",
    "        r2 = get_r2(ns, s)\n",
    "        payload_train[word] = {'r1': r1, 'r2': r2}\n",
    "\n",
    "    # Convert to DataFrame and transpose\n",
    "    payload_train_df = pd.DataFrame(payload_train).T\n",
    "    \n",
    "    return payload_train_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "severe_word_counts = Counter({\n",
    "    'jspindexmanager': 1, 'variety': 1, 'imodule': 1, 'iarchive': 1, 'javaartifacteditmodel': 1,\n",
    "    'aresource': 1, 'previous': 1, 'containers': 1, 'not_initialize': 1, 'inialize': 1, 'circumstances': 1,\n",
    "    'g': 1, 'match': 1, 'universalpathtransformer': 1, 'w': 1, 'publishers': 1, 'clear': 1, 'transaction': 1,\n",
    "    'not_ejbdoclet': 1, 'validatorstrategy': 1, 'unwanted': 1, 'activation': 1, 'serviceref': 1, 'dom': 1,\n",
    "    'choke': 1, 'earartifactedit': 1, 'getearartifacteditforread': 1, 'healess': 1, 'publisherdelegate': 1,\n",
    "    'delta': 1, 'kind': 1, 'doesnt': 1, 'erors': 1\n",
    "})\n",
    "nonsevere_word_counts = Counter({\n",
    "    'update': 46, 'need': 38, 'xml': 35, 'component': 23, 'file': 22, 'wst': 20, 'version': 18, 'jst': 18,\n",
    "    'wtp': 18, 'feature': 17, 'project': 16, 'java': 14, 'eclipse': 14, 'org': 14, 'jsp': 13, 'bundle': 12,\n",
    "    'change': 11, 'validation': 11, 'web': 11, 'page': 11, 'doc': 10, 'source': 10, 'build': 10, 'html': 10,\n",
    "    'remove': 10, 'number': 9, 'editor': 9, 'use': 9, 'api': 9, 'ee': 8, 'ui': 8, 'incorrect': 8, 'server': 8,\n",
    "    'add': 8, 'miss': 8, 'j': 7, 'type': 7, 'attribute': 7, 'service': 7, 'jar': 6, 'runtime': 6, 'string': 6,\n",
    "    'webtools': 6, 'plugin': 6, 'wizard': 6, 'common': 6, 'facet': 6, 'view': 5, 'jee': 5, 'delete': 5, 'edit': 5,\n",
    "    'new': 5, 'name': 5, 'plugins': 5, 'errors': 5, 'tag': 5, 'ear': 5, 'class': 5, 'wsdl': 5, 'invalid': 5,\n",
    "    'map': 5, 'cannot': 4, 'ws': 4, 'pom': 4, 'model': 4, 'move': 4, 'user': 4, 'npe': 4, 'not': 4\n",
    "})\n",
    "\n",
    "# Process the data\n",
    "payload_train_df = lexicon_preprocess(severe_word_counts, nonsevere_word_counts)\n",
    "print(payload_train_df)\n",
    "\n",
    "# Optionally, convert to dictionary for further processing or saving to JSON\n",
    "payload_train_dict = payload_train_df.to_dict(orient='index')\n",
    "print(json.dumps(payload_train_dict, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954565e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_onthresholds(severe_threshold, nonsevere_threshold, payload_train):\n",
    "    \"\"\"\n",
    "    Create dictionaries on each combination of severe and nonsevere threshold\n",
    "\n",
    "    Args:\n",
    "        severe_threshold: threshold set manually for severe from 0.1 to 1.0\n",
    "        nonsevere_threshold: threshold set manually for nonsevere from 0.1 to 1.0\n",
    "        payload_train: DataFrame having words with its counts as severe and nonsevere from the training dataset\n",
    "      \n",
    "    Returns: severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold\n",
    "    \"\"\"\n",
    "    severe_dictionary = {}\n",
    "    nonsevere_dictionary = {}\n",
    "\n",
    "    for keyy in payload_train.index:\n",
    "        # Check for 'r1' existence and value for severe threshold\n",
    "        if 'r1' in payload_train.columns and payload_train.at[keyy, 'r1'] >= severe_threshold:\n",
    "            severe_dictionary[keyy] = {'ratio': float(payload_train.at[keyy, 'r1'])}  # Store value and ratio as float\n",
    "\n",
    "        # Check for 'r2' existence and value for non-severe threshold\n",
    "        if 'r2' in payload_train.columns and payload_train.at[keyy, 'r2'] >= nonsevere_threshold:\n",
    "            nonsevere_dictionary[keyy] = {'ratio': float(payload_train.at[keyy, 'r2'])}  # Store value and ratio as float\n",
    "\n",
    "    print(\"severe_dictionary inside dictionary_onthresholds function\", severe_dictionary)\n",
    "    print(\"nonsevere_dictionary inside dictionary_onthresholds function\", nonsevere_dictionary)\n",
    "\n",
    "    return severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold\n",
    "\n",
    "# Example usage with given data\n",
    "import numpy as np\n",
    "\n",
    "winning_threshold = {'severe threshold': np.float64(0.1), 'non severe threshold': np.float64(0.8)}\n",
    "\n",
    "# Create example DataFrame for payload_train\n",
    "payload_train = pd.DataFrame({\n",
    "    'r1': [1.0, 0.5, 1.0, 1.0, 0.0],\n",
    "    'r2': [0.0, 0.5, 0.0, 0.0, 1.0]\n",
    "}, index=['axisclientgenerator', 'activation', 'ejbbean', 'synchhelpers', 'toggle'])\n",
    "\n",
    "severe_threshold = winning_threshold['severe threshold']\n",
    "nonsevere_threshold = winning_threshold['non severe threshold']\n",
    "\n",
    "severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold = dictionary_onthresholds(severe_threshold, nonsevere_threshold, payload_train)\n",
    "\n",
    "print(\"Severe Dictionary:\", severe_dictionary)\n",
    "print(\"Non-Severe Dictionary:\", nonsevere_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2044cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample data\n",
    "# data = {\n",
    "#     'Summary': [\n",
    "#         'Invalid version of EAR project not created when creating a new Application Client Project',\n",
    "#         'NullPointerException when accessing user Error details on the main dashboard',\n",
    "#         'UI freezes during file upload after recent update',\n",
    "#         'Crash occurs when opening large projects in Eclipse',\n",
    "#         'Validation fails for XML schema in version 1.5.2',\n",
    "#         'Application throws SAXParseException on parsing malformed XML',\n",
    "#         'Update Manager fails to detect new updates on server',\n",
    "#         'Error 500: Internal Server Error when accessing the Reports module',\n",
    "#         'Configuration settings not saved after restart',\n",
    "#         'Plugin dependency fails issues causing build failures'\n",
    "#     ],\n",
    "#     'Severity': [\n",
    "#         'Severe',\n",
    "#         'Severe',\n",
    "#         'NonSevere',\n",
    "#         'Severe',\n",
    "#         'NonSevere',\n",
    "#         'Severe',\n",
    "#         'Severe',\n",
    "#         'NonSevere',\n",
    "#         'NonSevere',\n",
    "#         'Severe'\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # Creating DataFrame\n",
    "# bugs_df = pd.DataFrame(data)\n",
    "# print(bugs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "training_data_df['Summary'] = training_data_df['Summary'].apply(lambda x: nlpsteps(x))\n",
    "validation_data_df['Summary'] = validation_data_df['Summary'].apply(lambda x: nlpsteps(x))\n",
    "\n",
    "# Combine tokenized data\n",
    "combined_testvalidation_data = pd.concat([training_data_df, validation_data_df])\n",
    "\n",
    "# Vectorize the combined tokenized summaries\n",
    "cv = CountVectorizer(max_features=100)\n",
    "X_train = cv.fit_transform(combined_testvalidation_data['Summary']).toarray()\n",
    "Y_train = combined_testvalidation_data['Severity'].apply(lambda x: 1 if x in ['Critical', 'Blocker'] else 0).values\n",
    "\n",
    "# Continue with training/testing...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27be99f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severe_dictionary inside dictionary_onthresholds function {'nullpointerexception': {'ratio': 1.0}, 'access': {'ratio': 0.5}, 'fail': {'ratio': 0.5}, 'open': {'ratio': 1.0}, 'issue': {'ratio': 1.0}, 'dashboard': {'ratio': 1.0}}\n",
      "nonsevere_dictionary inside dictionary_onthresholds function {'report': {'ratio': 1.0}, 'schema': {'ratio': 1.0}, 'internal': {'ratio': 1.0}}\n",
      "Severe Dictionary: {'nullpointerexception': {'ratio': 1.0}, 'access': {'ratio': 0.5}, 'fail': {'ratio': 0.5}, 'open': {'ratio': 1.0}, 'issue': {'ratio': 1.0}, 'dashboard': {'ratio': 1.0}}\n",
      "Non-Severe Dictionary: {'report': {'ratio': 1.0}, 'schema': {'ratio': 1.0}, 'internal': {'ratio': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "def dictionary_onthresholds(severe_threshold, nonsevere_threshold, payload_train):\n",
    "    \"\"\"\n",
    "    Create dictionaries on each combination of severe and nonsevere threshold\n",
    "\n",
    "    Args:\n",
    "        severe_threshold: threshold set manually for severe from 0.1 to 1.0\n",
    "        nonsevere_threshold: threshold set manually for nonsevere from 0.1 to 1.0\n",
    "        payload_train: DataFrame having words with its counts as severe and nonsevere from the training dataset\n",
    "      \n",
    "    Returns: severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold\n",
    "    \"\"\"      \n",
    "    severe_dictionary = {}\n",
    "    nonsevere_dictionary = {}\n",
    "\n",
    "    for keyy in payload_train.index:\n",
    "        # Check for 'r1' existence and value for severe threshold\n",
    "        if 'r1' in payload_train.columns and payload_train.at[keyy, 'r1'] >= severe_threshold:\n",
    "            severe_dictionary[keyy] = {'ratio': float(payload_train.at[keyy, 'r1'])}  # Store value and ratio as float\n",
    "\n",
    "        # Check for 'r2' existence and value for non-severe threshold\n",
    "        if 'r2' in payload_train.columns and payload_train.at[keyy, 'r2'] >= nonsevere_threshold:\n",
    "            nonsevere_dictionary[keyy] = {'ratio': float(payload_train.at[keyy, 'r2'])}  # Store value and ratio as float\n",
    "\n",
    "    print(\"severe_dictionary inside dictionary_onthresholds function\", severe_dictionary)\n",
    "    print(\"nonsevere_dictionary inside dictionary_onthresholds function\", nonsevere_dictionary)\n",
    "\n",
    "    return severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold\n",
    "\n",
    "# Example usage with sample data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame for payload_train\n",
    "payload_train = pd.DataFrame({\n",
    "    'r1': [0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.333333],\n",
    "    'r2': [1.0, 0.0, 1.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.0, 0.666667]\n",
    "}, index=['report', 'nullpointerexception', 'schema', 'access', 'fail', 'open', 'issue', 'internal', 'dashboard', 'error'])\n",
    "\n",
    "severe_threshold = 0.5\n",
    "nonsevere_threshold = 0.8\n",
    "\n",
    "severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold = dictionary_onthresholds(severe_threshold, nonsevere_threshold, payload_train)\n",
    "\n",
    "print(\"Severe Dictionary:\", severe_dictionary)\n",
    "print(\"Non-Severe Dictionary:\", nonsevere_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlpsteps(x):\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses a summary of a bug.\n",
    "\n",
    "    Args:\n",
    "        x (str): The summary text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: The processed text as a list of tokens after removing non-alphabetic characters, converting to lowercase,\n",
    "              lemmatizing words, and removing stopwords.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(x))\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Concatenate 'not' with the next word\n",
    "    processed_review = []\n",
    "    i = 0\n",
    "    while i < len(review):\n",
    "        if review[i] == 'not' and i + 1 < len(review):\n",
    "            processed_review.append('not_' + review[i + 1])\n",
    "            i += 2  # Skip the next word as it has been concatenated\n",
    "        else:\n",
    "            if review[i] not in all_stopwords:\n",
    "                processed_review.append(lemmatizer.lemmatize(review[i]))\n",
    "            i += 1\n",
    "\n",
    "    return processed_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d94e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample data\n",
    "trainingdata_tokenised = [\n",
    "    'jsp file not_indexed jsp model plugin not_activated'\n",
    "]\n",
    "\n",
    "# Initialize and apply CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train = cv.fit_transform(trainingdata_tokenised).toarray()\n",
    "\n",
    "# Display the document-term matrix\n",
    "feature_names = cv.get_feature_names_out()\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X_train, columns=feature_names)\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print('')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60bfa1ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/fatimaa/nltk_data'\n    - '/home/fatimaa/masters_project/DataAnalysis/Experiments/myenv/nltk_data'\n    - '/home/fatimaa/masters_project/DataAnalysis/Experiments/myenv/share/nltk_data'\n    - '/home/fatimaa/masters_project/DataAnalysis/Experiments/myenv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     43\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning dogs cats cat in the parks is the happiest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 44\u001b[0m preprocessed_text \u001b[38;5;241m=\u001b[39m \u001b[43mnlpsteps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(preprocessed_text)\n",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m, in \u001b[0;36mnlpsteps\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     35\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_stopwords]\n\u001b[1;32m     37\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m---> 38\u001b[0m lemmatized_tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token, \u001b[43mget_wordnet_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m filtered_tokens] \n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized_tokens)\n",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m, in \u001b[0;36mget_wordnet_pos\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pos_tag\n\u001b[0;32m---> 10\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m     11\u001b[0m tag_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADJ,\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mNOUN,\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mVERB,\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADV}\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tag_dict\u001b[38;5;241m.\u001b[39mget(tag, wordnet\u001b[38;5;241m.\u001b[39mNOUN)\n",
      "File \u001b[0;32m~/masters_project/DataAnalysis/Experiments/myenv/lib/python3.12/site-packages/nltk/tag/__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m~/masters_project/DataAnalysis/Experiments/myenv/lib/python3.12/site-packages/nltk/tag/__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m~/masters_project/DataAnalysis/Experiments/myenv/lib/python3.12/site-packages/nltk/tag/perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masters_project/DataAnalysis/Experiments/myenv/lib/python3.12/site-packages/nltk/tag/perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[0;32m~/masters_project/DataAnalysis/Experiments/myenv/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/fatimaa/nltk_data'\n    - '/home/fatimaa/masters_project/DataAnalysis/Experiments/myenv/nltk_data'\n    - '/home/fatimaa/masters_project/DataAnalysis/Experiments/myenv/share/nltk_data'\n    - '/home/fatimaa/masters_project/DataAnalysis/Experiments/myenv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to the format expected by WordNetLemmatizer.\"\"\"\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk import pos_tag\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def nlpsteps(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by handling negation, tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    removed_punctuation = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    removed_punctuation = removed_punctuation.lower()\n",
    "    tokens = removed_punctuation.split()\n",
    "\n",
    "    # Remove stopwords and 'not' is preserved\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in all_stopwords]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in filtered_tokens] \n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"running dogs cats cat in the parks is the happiest\"\n",
    "preprocessed_text = nlpsteps(text)\n",
    "print(preprocessed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
