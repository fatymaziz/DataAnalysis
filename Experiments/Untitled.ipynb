{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29341bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming get_r1 and get_r2 functions are defined elsewhere\n",
    "def get_r1(ns, s):\n",
    "    return s / (s + ns) if (s + ns) > 0 else 0\n",
    "\n",
    "def get_r2(ns, s):\n",
    "    return ns / (s + ns) if (s + ns) > 0 else 0\n",
    "\n",
    "def lexicon_preprocess(severe_word_counts, nonsevere_word_counts):\n",
    "    \"\"\"\n",
    "    Calculates the ratios for Severe and NonSevere categories.\n",
    "\n",
    "    Args:\n",
    "        severe_word_counts: Dictionary containing word counts for Severe category.\n",
    "        nonsevere_word_counts: Dictionary containing word counts for NonSevere category.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Contains words with their counts and ratios for Severe and NonSevere categories.\n",
    "    \"\"\"\n",
    "    # Combine word counts into a single dictionary\n",
    "    all_words = set(severe_word_counts.keys()).union(set(nonsevere_word_counts.keys()))\n",
    "    all_data = {word: {'Severe': severe_word_counts.get(word, 0), 'NonSevere': nonsevere_word_counts.get(word, 0)} for word in all_words}\n",
    "\n",
    "    # Calculate ratios and prepare payload\n",
    "    payload_train = {}\n",
    "    for word, counts in all_data.items():\n",
    "        ns = counts.get('NonSevere', 0)\n",
    "        s = counts.get('Severe', 0)\n",
    "        r1 = get_r1(ns, s)\n",
    "        r2 = get_r2(ns, s)\n",
    "        payload_train[word] = {'r1': r1, 'r2': r2}\n",
    "\n",
    "    # Convert to DataFrame and transpose\n",
    "    payload_train_df = pd.DataFrame(payload_train).T\n",
    "    \n",
    "    return payload_train_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "severe_word_counts = Counter({\n",
    "    'jspindexmanager': 1, 'variety': 1, 'imodule': 1, 'iarchive': 1, 'javaartifacteditmodel': 1,\n",
    "    'aresource': 1, 'previous': 1, 'containers': 1, 'not_initialize': 1, 'inialize': 1, 'circumstances': 1,\n",
    "    'g': 1, 'match': 1, 'universalpathtransformer': 1, 'w': 1, 'publishers': 1, 'clear': 1, 'transaction': 1,\n",
    "    'not_ejbdoclet': 1, 'validatorstrategy': 1, 'unwanted': 1, 'activation': 1, 'serviceref': 1, 'dom': 1,\n",
    "    'choke': 1, 'earartifactedit': 1, 'getearartifacteditforread': 1, 'healess': 1, 'publisherdelegate': 1,\n",
    "    'delta': 1, 'kind': 1, 'doesnt': 1, 'erors': 1\n",
    "})\n",
    "nonsevere_word_counts = Counter({\n",
    "    'update': 46, 'need': 38, 'xml': 35, 'component': 23, 'file': 22, 'wst': 20, 'version': 18, 'jst': 18,\n",
    "    'wtp': 18, 'feature': 17, 'project': 16, 'java': 14, 'eclipse': 14, 'org': 14, 'jsp': 13, 'bundle': 12,\n",
    "    'change': 11, 'validation': 11, 'web': 11, 'page': 11, 'doc': 10, 'source': 10, 'build': 10, 'html': 10,\n",
    "    'remove': 10, 'number': 9, 'editor': 9, 'use': 9, 'api': 9, 'ee': 8, 'ui': 8, 'incorrect': 8, 'server': 8,\n",
    "    'add': 8, 'miss': 8, 'j': 7, 'type': 7, 'attribute': 7, 'service': 7, 'jar': 6, 'runtime': 6, 'string': 6,\n",
    "    'webtools': 6, 'plugin': 6, 'wizard': 6, 'common': 6, 'facet': 6, 'view': 5, 'jee': 5, 'delete': 5, 'edit': 5,\n",
    "    'new': 5, 'name': 5, 'plugins': 5, 'errors': 5, 'tag': 5, 'ear': 5, 'class': 5, 'wsdl': 5, 'invalid': 5,\n",
    "    'map': 5, 'cannot': 4, 'ws': 4, 'pom': 4, 'model': 4, 'move': 4, 'user': 4, 'npe': 4, 'not': 4\n",
    "})\n",
    "\n",
    "# Process the data\n",
    "payload_train_df = lexicon_preprocess(severe_word_counts, nonsevere_word_counts)\n",
    "print(payload_train_df)\n",
    "\n",
    "# Optionally, convert to dictionary for further processing or saving to JSON\n",
    "payload_train_dict = payload_train_df.to_dict(orient='index')\n",
    "print(json.dumps(payload_train_dict, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954565e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_onthresholds(severe_threshold, nonsevere_threshold, payload_train):\n",
    "    \"\"\"\n",
    "    Create dictionaries on each combination of severe and nonsevere threshold\n",
    "\n",
    "    Args:\n",
    "        severe_threshold: threshold set manually for severe from 0.1 to 1.0\n",
    "        nonsevere_threshold: threshold set manually for nonsevere from 0.1 to 1.0\n",
    "        payload_train: DataFrame having words with its counts as severe and nonsevere from the training dataset\n",
    "      \n",
    "    Returns: severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold\n",
    "    \"\"\"\n",
    "    severe_dictionary = {}\n",
    "    nonsevere_dictionary = {}\n",
    "\n",
    "    for keyy in payload_train.index:\n",
    "        # Check for 'r1' existence and value for severe threshold\n",
    "        if 'r1' in payload_train.columns and payload_train.at[keyy, 'r1'] >= severe_threshold:\n",
    "            severe_dictionary[keyy] = {'ratio': float(payload_train.at[keyy, 'r1'])}  # Store value and ratio as float\n",
    "\n",
    "        # Check for 'r2' existence and value for non-severe threshold\n",
    "        if 'r2' in payload_train.columns and payload_train.at[keyy, 'r2'] >= nonsevere_threshold:\n",
    "            nonsevere_dictionary[keyy] = {'ratio': float(payload_train.at[keyy, 'r2'])}  # Store value and ratio as float\n",
    "\n",
    "    print(\"severe_dictionary inside dictionary_onthresholds function\", severe_dictionary)\n",
    "    print(\"nonsevere_dictionary inside dictionary_onthresholds function\", nonsevere_dictionary)\n",
    "\n",
    "    return severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold\n",
    "\n",
    "# Example usage with given data\n",
    "import numpy as np\n",
    "\n",
    "winning_threshold = {'severe threshold': np.float64(0.1), 'non severe threshold': np.float64(0.8)}\n",
    "\n",
    "# Create example DataFrame for payload_train\n",
    "payload_train = pd.DataFrame({\n",
    "    'r1': [1.0, 0.5, 1.0, 1.0, 0.0],\n",
    "    'r2': [0.0, 0.5, 0.0, 0.0, 1.0]\n",
    "}, index=['axisclientgenerator', 'activation', 'ejbbean', 'synchhelpers', 'toggle'])\n",
    "\n",
    "severe_threshold = winning_threshold['severe threshold']\n",
    "nonsevere_threshold = winning_threshold['non severe threshold']\n",
    "\n",
    "severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold = dictionary_onthresholds(severe_threshold, nonsevere_threshold, payload_train)\n",
    "\n",
    "print(\"Severe Dictionary:\", severe_dictionary)\n",
    "print(\"Non-Severe Dictionary:\", nonsevere_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2044cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample data\n",
    "# data = {\n",
    "#     'Summary': [\n",
    "#         'Invalid version of EAR project not created when creating a new Application Client Project',\n",
    "#         'NullPointerException when accessing user Error details on the main dashboard',\n",
    "#         'UI freezes during file upload after recent update',\n",
    "#         'Crash occurs when opening large projects in Eclipse',\n",
    "#         'Validation fails for XML schema in version 1.5.2',\n",
    "#         'Application throws SAXParseException on parsing malformed XML',\n",
    "#         'Update Manager fails to detect new updates on server',\n",
    "#         'Error 500: Internal Server Error when accessing the Reports module',\n",
    "#         'Configuration settings not saved after restart',\n",
    "#         'Plugin dependency fails issues causing build failures'\n",
    "#     ],\n",
    "#     'Severity': [\n",
    "#         'Severe',\n",
    "#         'Severe',\n",
    "#         'NonSevere',\n",
    "#         'Severe',\n",
    "#         'NonSevere',\n",
    "#         'Severe',\n",
    "#         'Severe',\n",
    "#         'NonSevere',\n",
    "#         'NonSevere',\n",
    "#         'Severe'\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # Creating DataFrame\n",
    "# bugs_df = pd.DataFrame(data)\n",
    "# print(bugs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27be99f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severe_dictionary inside dictionary_onthresholds function {'nullpointerexception': {'ratio': 1.0}, 'access': {'ratio': 0.5}, 'fail': {'ratio': 0.5}, 'open': {'ratio': 1.0}, 'issue': {'ratio': 1.0}, 'dashboard': {'ratio': 1.0}}\n",
      "nonsevere_dictionary inside dictionary_onthresholds function {'report': {'ratio': 1.0}, 'schema': {'ratio': 1.0}, 'internal': {'ratio': 1.0}}\n",
      "Severe Dictionary: {'nullpointerexception': {'ratio': 1.0}, 'access': {'ratio': 0.5}, 'fail': {'ratio': 0.5}, 'open': {'ratio': 1.0}, 'issue': {'ratio': 1.0}, 'dashboard': {'ratio': 1.0}}\n",
      "Non-Severe Dictionary: {'report': {'ratio': 1.0}, 'schema': {'ratio': 1.0}, 'internal': {'ratio': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "def dictionary_onthresholds(severe_threshold, nonsevere_threshold, payload_train):\n",
    "    \"\"\"\n",
    "    Create dictionaries on each combination of severe and nonsevere threshold\n",
    "\n",
    "    Args:\n",
    "        severe_threshold: threshold set manually for severe from 0.1 to 1.0\n",
    "        nonsevere_threshold: threshold set manually for nonsevere from 0.1 to 1.0\n",
    "        payload_train: DataFrame having words with its counts as severe and nonsevere from the training dataset\n",
    "      \n",
    "    Returns: severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold\n",
    "    \"\"\"      \n",
    "    severe_dictionary = {}\n",
    "    nonsevere_dictionary = {}\n",
    "\n",
    "    for keyy in payload_train.index:\n",
    "        # Check for 'r1' existence and value for severe threshold\n",
    "        if 'r1' in payload_train.columns and payload_train.at[keyy, 'r1'] >= severe_threshold:\n",
    "            severe_dictionary[keyy] = {'ratio': float(payload_train.at[keyy, 'r1'])}  # Store value and ratio as float\n",
    "\n",
    "        # Check for 'r2' existence and value for non-severe threshold\n",
    "        if 'r2' in payload_train.columns and payload_train.at[keyy, 'r2'] >= nonsevere_threshold:\n",
    "            nonsevere_dictionary[keyy] = {'ratio': float(payload_train.at[keyy, 'r2'])}  # Store value and ratio as float\n",
    "\n",
    "    print(\"severe_dictionary inside dictionary_onthresholds function\", severe_dictionary)\n",
    "    print(\"nonsevere_dictionary inside dictionary_onthresholds function\", nonsevere_dictionary)\n",
    "\n",
    "    return severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold\n",
    "\n",
    "# Example usage with sample data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame for payload_train\n",
    "payload_train = pd.DataFrame({\n",
    "    'r1': [0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.333333],\n",
    "    'r2': [1.0, 0.0, 1.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.0, 0.666667]\n",
    "}, index=['report', 'nullpointerexception', 'schema', 'access', 'fail', 'open', 'issue', 'internal', 'dashboard', 'error'])\n",
    "\n",
    "severe_threshold = 0.5\n",
    "nonsevere_threshold = 0.8\n",
    "\n",
    "severe_dictionary, nonsevere_dictionary, severe_threshold, nonsevere_threshold = dictionary_onthresholds(severe_threshold, nonsevere_threshold, payload_train)\n",
    "\n",
    "print(\"Severe Dictionary:\", severe_dictionary)\n",
    "print(\"Non-Severe Dictionary:\", nonsevere_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlpsteps(x):\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses a summary of a bug.\n",
    "\n",
    "    Args:\n",
    "        x (str): The summary text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: The processed text as a list of tokens after removing non-alphabetic characters, converting to lowercase,\n",
    "              lemmatizing words, and removing stopwords.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(x))\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Concatenate 'not' with the next word\n",
    "    processed_review = []\n",
    "    i = 0\n",
    "    while i < len(review):\n",
    "        if review[i] == 'not' and i + 1 < len(review):\n",
    "            processed_review.append('not_' + review[i + 1])\n",
    "            i += 2  # Skip the next word as it has been concatenated\n",
    "        else:\n",
    "            if review[i] not in all_stopwords:\n",
    "                processed_review.append(lemmatizer.lemmatize(review[i]))\n",
    "            i += 1\n",
    "\n",
    "    return processed_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d94e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample data\n",
    "trainingdata_tokenised = [\n",
    "    'jsp file not_indexed jsp model plugin not_activated'\n",
    "]\n",
    "\n",
    "# Initialize and apply CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train = cv.fit_transform(trainingdata_tokenised).toarray()\n",
    "\n",
    "# Display the document-term matrix\n",
    "feature_names = cv.get_feature_names_out()\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X_train, columns=feature_names)\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print('')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfa1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
