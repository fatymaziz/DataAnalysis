{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bba2523-fadf-491e-951e-760194936e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Summary   Severity\n",
      "0    Cannot create Java EE artifacts: servlet, bean...     Severe\n",
      "1                         Discovery plugin regressions     Severe\n",
      "2    P2 doesn't install all features, compared to U...     Severe\n",
      "3                 Versioning issues on latet 3.1 build     Severe\n",
      "4    [doc] \"New and Noteworthy\" document in JSF Use...     Severe\n",
      "..                                                 ...        ...\n",
      "495             a number of bundles need version bumps  NonSevere\n",
      "496  Run on Server a JSP  fails on web 2.5 modules ...  NonSevere\n",
      "497  CHKJ3027E:  Invalid Exception Type java.lang.T...  NonSevere\n",
      "498      small legal related fix needed for JPA schema  NonSevere\n",
      "499  [EclipseLink] eclipselink 2.3 schemas not up t...  NonSevere\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Wordlist with Ratios:\n",
      "                                     r1        r2\n",
      "j                              0.785714  0.214286\n",
      "pane                           0.000000  1.000000\n",
      "attach                         0.000000  1.000000\n",
      "unsupportedoperationexception  1.000000  0.000000\n",
      "quote                          0.000000  1.000000\n",
      "...                                 ...       ...\n",
      "earfileimpl                    1.000000  0.000000\n",
      "optimization                   0.000000  1.000000\n",
      "functional                     1.000000  0.000000\n",
      "register                       1.000000  0.000000\n",
      "maintenance                    0.333333  0.666667\n",
      "\n",
      "[1088 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import collections\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def nlpsteps(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by handling negation, tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    removed_punctuation = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    removed_punctuation = removed_punctuation.lower()\n",
    "    tokens = removed_punctuation.split()\n",
    "\n",
    "    # Remove stopwords and 'not' is preserved\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in all_stopwords]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in filtered_tokens]  # Use 'v' for verbs\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def convert(corpus_trainingdata):\n",
    "    \"\"\"\n",
    "    Data after preprocessing splitting into separate words\n",
    "\n",
    "    Args:\n",
    "        corpus_trainingdata: Preprocessed data of the training dataset\n",
    "     \n",
    "    Returns: Splitted words\n",
    "    \"\"\"\n",
    "    return [i for item in corpus_trainingdata for i in item.split()]\n",
    "\n",
    "def getwordcounts(splittedWords):\n",
    "    occurrences = collections.Counter(splittedWords)\n",
    "    return occurrences\n",
    "\n",
    "def get_r1(ns, s):\n",
    "    \"\"\"\n",
    "    Calculate ratio for severe\n",
    "\n",
    "    Args:\n",
    "        ns:Number of counts for a word as nonsevere\n",
    "        s: Number of counts for a word as severe\n",
    "      \n",
    "    Returns: severe ratio for a given word\n",
    "    \"\"\"\n",
    "    return s / (s + ns)\n",
    "\n",
    "def get_r2(ns, s):\n",
    "    \"\"\"\n",
    "    Calculate ratio for nonsevere\n",
    "\n",
    "    Args:\n",
    "        ns:Number of counts for a word as nonsevere\n",
    "        s: Number of counts for a word as severe\n",
    "      \n",
    "    Returns: non-severe ratio for a given word\n",
    "    \"\"\"\n",
    "    return ns / (s + ns)\n",
    "\n",
    "def get_distribution(training_data_df):\n",
    "    \"\"\"\n",
    "    Collects word counts for Severe and NonSevere categories.\n",
    "\n",
    "    Args:\n",
    "        training_data_df: DataFrame containing the training dataset with 'Summary' and 'Severity' columns.\n",
    "\n",
    "    Returns:\n",
    "        dict, dict: Two dictionaries containing word counts for Severe and NonSevere categories.\n",
    "    \"\"\"\n",
    "    # Preprocess the summary text in the dataset using .loc\n",
    "    training_data_df.loc[:, 'Summary'] = training_data_df['Summary'].apply(lambda x: nlpsteps(x))\n",
    "    \n",
    "    # Separate the dataset into Severe and NonSevere\n",
    "    severe_df = training_data_df[training_data_df['Severity'] == 'Severe']\n",
    "    nonsevere_df = training_data_df[training_data_df['Severity'] == 'NonSevere']\n",
    "    \n",
    "    # Convert summaries into lists of words\n",
    "    severe_words = convert(severe_df['Summary'])\n",
    "    nonsevere_words = convert(nonsevere_df['Summary'])\n",
    "    \n",
    "    # Get word counts\n",
    "    severe_word_counts = getwordcounts(severe_words)\n",
    "    nonsevere_word_counts = getwordcounts(nonsevere_words)\n",
    "    \n",
    "    return severe_word_counts, nonsevere_word_counts\n",
    "\n",
    "def calculate_ratios(severe_word_counts, nonsevere_word_counts):\n",
    "    \"\"\"\n",
    "    Calculates the ratios for Severe and NonSevere categories.\n",
    "\n",
    "    Args:\n",
    "        severe_word_counts: Dictionary containing word counts for Severe category.\n",
    "        nonsevere_word_counts: Dictionary containing word counts for NonSevere category.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Contains words with their counts and ratios for Severe and NonSevere categories.\n",
    "    \"\"\"\n",
    "    # Combine word counts into a single dictionary\n",
    "    all_words = set(severe_word_counts.keys()).union(set(nonsevere_word_counts.keys()))\n",
    "    all_data = {word: {'Severe': severe_word_counts.get(word, 0), 'NonSevere': nonsevere_word_counts.get(word, 0)} for word in all_words}\n",
    "\n",
    "    # Calculate ratios and prepare payload\n",
    "    payload_train = {}\n",
    "    for word, counts in all_data.items():\n",
    "        ns = counts.get('NonSevere', 0)\n",
    "        s = counts.get('Severe', 0)\n",
    "        r1 = get_r1(ns, s)\n",
    "        r2 = get_r2(ns, s)\n",
    "        payload_train[word] = {'r1': r1, 'r2': r2}\n",
    "    \n",
    "    payload_train_df = pd.DataFrame(payload_train).T\n",
    "    return payload_train_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming your data is in a CSV file named 'data.csv'\n",
    "    bugs_eclipse = pd.read_csv('bugs_eclipse.csv')\n",
    "    \n",
    "    bugs_eclipse['Type'] = np.where(bugs_eclipse['Severity'] == 'enhancement', \"enhancement\", \"defect\")\n",
    "    bugs_df = pd.concat([bugs_eclipse])\n",
    "\n",
    "    # Dropped rows with severity level '--'\n",
    "    bugs_df = bugs_df[bugs_df[\"Severity\"].str.contains(\"--\") == False].reset_index()\n",
    "\n",
    "    #Dropped rows with Type \"Enhancement\" and \"Task\" because they are not a bug but a new feature\n",
    "    indexSevere = bugs_df[ (bugs_df['Type'] == 'enhancement') & (bugs_df['Type'] == 'enhancement') ].index\n",
    "    bugs_df.drop(indexSevere , inplace=True)\n",
    "\n",
    "    indexSevere = bugs_df[ (bugs_df['Type'] == 'task') & (bugs_df['Type'] == 'task') ].index\n",
    "    bugs_df.drop(indexSevere , inplace=True)\n",
    "\n",
    "    # Catagorize the severity level into Severe and NonSevere to make it a binary problem\n",
    "    bugs_df.loc[bugs_df[\"Severity\"] == \"blocker\", \"Severity\"] = 'Severe'\n",
    "    bugs_df.loc[bugs_df[\"Severity\"] == \"critical\", \"Severity\"] = 'Severe'\n",
    "    bugs_df.loc[bugs_df[\"Severity\"] == \"major\", \"Severity\"] = 'Severe'\n",
    "    bugs_df.loc[bugs_df[\"Severity\"] == \"S1\", \"Severity\"] = 'Severe'\n",
    "    bugs_df.loc[bugs_df[\"Severity\"] == \"S2\", \"Severity\"] = 'Severe'\n",
    "    bugs_df.loc[bugs_df[\"Severity\"] == \"S3\", \"Severity\"]= 'NonSevere'\n",
    "    bugs_df.loc[bugs_df[\"Severity\"] == \"normal\", \"Severity\"] = 'NonSevere'\n",
    "    bugs_df.loc[bugs_df[\"Severity\"] == \"minor\", \"Severity\"] = 'NonSevere'\n",
    "    bugs_df.loc[bugs_df[\"Severity\"] == \"trivial\", \"Severity\"] = 'NonSevere'\n",
    "    bugs_df.loc[bugs_df[\"Severity\"] == \"S4\", \"Severity\"] = 'NonSevere'\n",
    "\n",
    "    bugs_df = bugs_df.head(500)\n",
    "    # Ensure the dataset only contains the 'Summary' and 'Severity' columns\n",
    "    df = bugs_df[['Summary', 'Severity']]\n",
    "    print(df)\n",
    "    \n",
    "    # Get word counts\n",
    "    severe_word_counts, nonsevere_word_counts = get_distribution(df)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    payload_train_df = calculate_ratios(severe_word_counts, nonsevere_word_counts)\n",
    "    print(\"Wordlist with Ratios:\")\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    print(payload_train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462b4d8-16b0-4906-a884-419170a28d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671359ad-c4b1-4840-ac3d-3bf9ae2241f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_eclipse = pd.read_csv(\"bugs_eclipse.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd67f7-a754-447e-8c51-b89ce730e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_eclipse['Type'] = np.where(bugs_eclipse['Severity'] == 'enhancement', \"enhancement\", \"defect\")\n",
    "bugs_df = pd.concat([bugs_eclipse])\n",
    "\n",
    "# Dropped rows with severity level '--'\n",
    "bugs_df = bugs_df[bugs_df[\"Severity\"].str.contains(\"--\")==False].reset_index()\n",
    "\n",
    "#Dropped rows with Type \"Enhancement\" and \"Task\" because they are not a bug but a new feature\n",
    "indexSevere = bugs_df[ (bugs_df['Type'] == 'enhancement') & (bugs_df['Type'] == 'enhancement') ].index\n",
    "bugs_df.drop(indexSevere , inplace=True)\n",
    "\n",
    "indexSevere = bugs_df[ (bugs_df['Type'] == 'task') & (bugs_df['Type'] == 'task') ].index\n",
    "bugs_df.drop(indexSevere , inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#Catagorise the severity level into a Severe and Non Severe to make it a binary problem\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"blocker\", \"Severity\"] = 'Severe'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"critical\", \"Severity\"] = 'Severe'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"major\", \"Severity\"] = 'Severe'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"S1\", \"Severity\"] = 'Severe'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"S2\", \"Severity\"] = 'Severe'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"S3\", \"Severity\"] = 'NonSevere'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"normal\", \"Severity\"] = 'NonSevere'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"minor\", \"Severity\"] = 'NonSevere'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"trivial\", \"Severity\"] = 'NonSevere'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"S4\", \"Severity\"] = 'NonSevere'\n",
    "\n",
    "bugs_df = bugs_df.tail(50)\n",
    "# print(bugs_df)\n",
    "print(\"total bugs\", len(bugs_df))\n",
    "severerity = bugs_df['Severity'].value_counts()\n",
    "print(severerity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1d193-fb0a-42e8-9742-248ed0c1eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482df686-fc67-4080-97c9-5115011dce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_eclipse_length = len(bugs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448171e1-11b5-4359-bcd6-b8c104e438d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_eclipse_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10058ef0-4031-44de-ad7f-82c933ba6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlpsteps(x):\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses a summary of a bug.\n",
    "\n",
    "    Args:\n",
    "        x (str): The summary text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after removing non-alphabetic characters, converting to lowercase,\n",
    "             lemmatizing words, and removing stopwords.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(x))\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove stopwords and lemmatize words\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if word not in all_stopwords]\n",
    "\n",
    "    # Join the processed words back into a sentence\n",
    "    review = ' '.join(review)\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913b39a-acc4-4240-9536-291b6adcdf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(corpus_trainingdata):\n",
    "    \"\"\"\n",
    "    Data after preprocessing splitting into separate words\n",
    "\n",
    "    Args:\n",
    "        corpus_trainingdata: Preprocessed data of the training dataset\n",
    "     \n",
    "    Returns: Splitted words\n",
    "    \"\"\"\n",
    "#     print(\"DEMO--------------------------Corpus---------------------------\")\n",
    "#     print(corpus_trainingdata)\n",
    "    return ([i for item in corpus_trainingdata for i in item.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ecfbe-c405-4d56-840e-bda115ea5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of each words in the corpus\n",
    "def getwordcounts(splittedWords):\n",
    "    occurrences = collections.Counter(splittedWords)\n",
    "    return occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae27ab-bfb8-454b-8aa3-476e2e0e8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess the summary text in the dataset\n",
    "bugs_df['Summary'] = bugs_df['Summary'].apply(lambda x: nlpsteps(x))\n",
    "\n",
    "# Separate the dataset into Severe and NonSevere\n",
    "severe_df = bugs_df[bugs_df['Severity'] == 'Severe']\n",
    "nonsevere_df = bugs_df[bugs_df['Severity'] == 'NonSevere']\n",
    "\n",
    "# Convert summaries into lists of words\n",
    "severe_words = convert(severe_df['Summary'])\n",
    "nonsevere_words = convert(nonsevere_df['Summary'])\n",
    "\n",
    "# Get word counts\n",
    "severe_word_counts = getwordcounts(severe_words)\n",
    "nonsevere_word_counts = getwordcounts(nonsevere_words)\n",
    "\n",
    "# Convert Counter objects to lists of tuples\n",
    "severe_list = list(severe_word_counts.items())\n",
    "nonsevere_list = list(nonsevere_word_counts.items())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4bea72-c179-41fc-9423-7a0e4227c0ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "severe_list, nonsevere_list = get_distribution(bugs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd77927-c8c3-4806-b2a8-0e5e5bb8a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(val,bugs_eclipse):\n",
    "    \"\"\"\n",
    "    Data after preprocessing splitting into separate words\n",
    "\n",
    "    Args:\n",
    "        val: Preprocessed data of the training dataset\n",
    "        training_data_df: training dataset dataframe\n",
    "      \n",
    "    Returns: Splitted words\n",
    "    \"\"\"\n",
    "    bugs_eclipse['Summary'] = bugs_eclipse['Summary'].apply(lambda x: nlpsteps(x))\n",
    "    records = bugs_eclipse[\n",
    "        bugs_eclipse[\"Summary\"].str.contains(val)\n",
    "    ]\n",
    "    \n",
    "    if len(records) > 0:\n",
    "        res = bugs_eclipse[\n",
    "            training_data_df[\"Summary\"].str.contains(val)\n",
    "        ][\"Severity\"].value_counts(dropna=False)\n",
    "        return dict(res)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df4d99-2daa-4af9-b957-91e7042cc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_preprocess(trainingdataset_length,training_data_df):\n",
    "    \"\"\"\n",
    "    Create wordlists for severe and non severe from the preprocessed training dataset \n",
    "\n",
    "    Args:\n",
    "        trainingdataset_length: size of training dataset\n",
    "        training_data_df: training dataset dataframe\n",
    "      \n",
    "    Returns: a wordlist that has words from training dataset with its counts for severe and nonsevere\n",
    "    \"\"\"\n",
    "    # print(\"trainingdataset_length\",trainingdataset_length)\n",
    "    # print(\"training_data_df\",training_data_df)\n",
    "    corpus_trainingdata = []\n",
    "    all_data_df_ = []\n",
    "       \n",
    "    for i in range(0,trainingdataset_length):\n",
    "        review = nlpsteps(str(training_data_df['Summary'][i]))\n",
    "        corpus_trainingdata.append(review)\n",
    "   \n",
    "\n",
    "#     #Split words from the corpus\n",
    "#     splittedWords = convert(corpus_trainingdata)\n",
    "# #     print(\"splittedWords---------------\", splittedWords)\n",
    "    \n",
    "#     splitted_words=getwordcounts(splittedWords)\n",
    "\n",
    "#     #Converted collection.counter into dictionary\n",
    "#     splitted_words_dict = dict(splitted_words)\n",
    "\n",
    "#     keys = splitted_words_dict.keys()\n",
    "    \n",
    "#     all_data = {}\n",
    "#     for key in keys:\n",
    "#         res = get_distribution(key,training_data_df)\n",
    "#         if res:\n",
    "#             all_data[key] = res\n",
    "#             all_data\n",
    "#             all_data_df = pd.DataFrame(all_data)\n",
    "       \n",
    "#             print(\"--------------wordlists for severe and non-severe------------------------\")\n",
    "         \n",
    "#             pd.set_option('display.max_columns', None)\n",
    "#             print(all_data_df)\n",
    "        \n",
    "\n",
    "    return all_data_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db570e-a861-4180-af5b-948a11d45093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e7ab3-2f03-48da-a346-a19b2ac3b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_train = lexicon_preprocess(bugs_eclipse_length, bugs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319de6a9-1408-439d-a41c-a13e8af60ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fe5cf-900e-49d6-9df9-01afb380fe2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
