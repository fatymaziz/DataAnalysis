{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fatimaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'dogs', 'cats', 'cat', 'in', 'the', 'parks', 'is', 'the', 'happiest', 'errors', 'bugs', 'freezing']\n",
      "run dog cat cat park happiest errors bug freeze\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')  # Don't forget to download stopwords!\n",
    "\n",
    "def nlpsteps(text):\n",
    "\n",
    "    # Remove punctuation\n",
    "    removed_punctuation = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    removed_punctuation = removed_punctuation.lower()\n",
    "    tokens = removed_punctuation.split()\n",
    "\n",
    "    print(tokens)\n",
    "    \n",
    "    # # Tokenize\n",
    "    # tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and 'not' is preserved\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in all_stopwords]\n",
    "    # print(\"filtered\", filtered_tokens)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in filtered_tokens]  # Use 'v' for verbs\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"running dogs cats cat in the parks is the happiest errors bugs freezing\"\n",
    "preprocessed_text = nlpsteps(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the wordnet data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Download the part-of-speech tagger\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Download the stopwords list\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Set the NLTK data directory\n",
    "nltk.data.path.append('/home/fatimaa/nltk_data')\n",
    "\n",
    "# Ensure the required packages are available\n",
    "nltk.download('wordnet', download_dir='/home/fatimaa/nltk_data')\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='/home/fatimaa/nltk_data')\n",
    "nltk.download('stopwords', download_dir='/home/fatimaa/nltk_data')\n",
    "\n",
    "# Your existing code here\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to the format expected by WordNetLemmatizer.\"\"\"\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk import pos_tag\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def nlpsteps(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by handling negation, tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    removed_punctuation = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    removed_punctuation = removed_punctuation.lower()\n",
    "    tokens = removed_punctuation.split()\n",
    "\n",
    "    # Remove stopwords and 'not' is preserved\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in all_stopwords]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in filtered_tokens] \n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"running dogs cats cat in the parks is the happiest\"\n",
    "preprocessed_text = nlpsteps(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to the format expected by WordNetLemmatizer.\"\"\"\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk import pos_tag\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def nlpsteps(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by handling negation, tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    removed_punctuation = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    removed_punctuation = removed_punctuation.lower()\n",
    "    tokens = removed_punctuation.split()\n",
    "\n",
    "    # Remove stopwords and 'not' is preserved\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in all_stopwords]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in filtered_tokens] \n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "text = \"running dogs cats cat in the parks is the happiest\"\n",
    "preprocessed_text = nlpsteps(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlpsteps(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by handling negation, tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Handle negation and remove stopwords\n",
    "    processed_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] == 'not' and i + 1 < len(tokens) and tokens[i + 1] not in all_stopwords:\n",
    "            lemmatized_next_word = lemmatizer.lemmatize(tokens[i + 1], pos='v')\n",
    "            processed_tokens.append('not_' + lemmatized_next_word)\n",
    "            i += 2  # Skip the next word as it has been concatenated\n",
    "        else:\n",
    "            if tokens[i] not in all_stopwords:\n",
    "                lemmatized_word = lemmatizer.lemmatize(tokens[i], pos='v')\n",
    "                processed_tokens.append(lemmatized_word)\n",
    "            i += 1\n",
    "\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the preprocess function\n",
    "def nlpsteps(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    print(tokens)\n",
    "    \n",
    "    # Remove stopwords and preserve 'not'\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    \n",
    "    # Filter out stopwords and punctuation\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in all_stopwords and token not in string.punctuation]\n",
    "    \n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower(), pos='v') for token in filtered_tokens]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Example usage\n",
    "text = \"Move old unsupported WTP releases from download.eclipse.org to archive.eclipse.org words cannot\"\n",
    "preprocessed_text = nlpsteps(text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Example usage with \"create,\" \"creating,\" and \"creates\"\n",
    "text = \"create creating creates\"\n",
    "processed_text = preprocess_text(text)\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nlpsteps(x):\n",
    "    \"\"\"\n",
    "    Tokenizes and preprocesses a summary of a bug.\n",
    "\n",
    "    Args:\n",
    "        x (str): The summary text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after removing non-alphabetic characters, converting to lowercase,\n",
    "             lemmatizing words, and removing stopwords.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(x))\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    # Initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove stopwords and lemmatize words\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if word not in all_stopwords]\n",
    "\n",
    "    # Join the processed words back into a sentence\n",
    "    review = ' '.join(review)\n",
    "    return review\n",
    "\n",
    "text = \"they love to play, they played yesterday as well. They also play with another cats, playing\"\n",
    "preprocessed_text = nlpsteps(text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the default list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')  # Don't forget to download stopwords!\n",
    "\n",
    "def nlpsteps(text):\n",
    "\n",
    "    # Remove punctuation\n",
    "    removed_punctuation = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    removed_punctuation = removed_punctuation.lower()\n",
    "    tokens = removed_punctuation.split()\n",
    "\n",
    "    print(tokens)\n",
    "    \n",
    "    # # Tokenize\n",
    "    # tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and 'not' is preserved\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    all_stopwords.remove('not')\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in all_stopwords]\n",
    "    # print(\"filtered\", filtered_tokens)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in filtered_tokens]  # Use 'v' for verbs\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Example DataFrame\n",
    "training_data_df = pd.DataFrame({\n",
    "    'Summary': ['jsp file not indexed jsp model plugin not activated'],\n",
    "    'Severity': ['Severe']\n",
    "})\n",
    "\n",
    "# Apply the nlpsteps function to the 'Summary' column\n",
    "training_data_df.loc[:, 'Summary'] = training_data_df['Summary'].apply(lambda x: nlpsteps(x))\n",
    "\n",
    "# Initialize and apply CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train = cv.fit_transform(training_data_df['Summary']).toarray()\n",
    "\n",
    "# Display the document-term matrix\n",
    "feature_names = cv.get_feature_names_out()\n",
    "df = pd.DataFrame(X_train, columns=feature_names)\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print('')\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
