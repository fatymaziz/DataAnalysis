{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926976ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete this cell after the experiment and uncomment below cells------------------****--------------------\n",
    "bugs_df= pd.read_csv(\"bugs_firefox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bugs_eclipse = pd.read_csv(\"bugs_eclipse.csv\")\n",
    "# bugs_firefox= pd.read_csv(\"bugs_firefox.csv\")\n",
    "# bugs_calendar= pd.read_csv(\"bugs_calendar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bugs_eclipse['Type'] = np.where(bugs_eclipse['Severity'] == 'enhancement', \"enhancement\", \"defect\")\n",
    "# bugs_eclipse.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178382cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(bugs_firefox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(bugs_calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(bugs_eclipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bugs_df = pd.concat([bugs_firefox,bugs_calendar,bugs_eclipse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bfb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5818ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find duplicates in both bug list \n",
    "duplicate = bugs_df[bugs_df.duplicated('Summary')]\n",
    "duplicate.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cadfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find duplicates in both bug list \n",
    "rslt_df = bugs_df.sort_values(by = ['Bug ID', 'Summary'])\n",
    "rslt_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c60b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bugs_df= bugs_df.drop_duplicates(subset=['Bug ID', 'Summary'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b07d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(firefoxbugs), len(eclispsebugs), len(calendarbugs))\n",
    "\n",
    "len(bugs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8064f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'BugCombinedData.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "#bugs_df.to_excel(file_name)\n",
    "print('DataFrame is written to Excel File successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd39251",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba54211",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6817f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bugs_df=bugs_df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbfa108",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bugs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df = bugs_df[bugs_df[\"Severity\"].str.contains(\"--\")==False].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a226dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bugs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8020e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bugs_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff35dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df['Severity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropped rows with Type \"Enhancement\" because they are not a bug but a new feature\n",
    "indexSevere = bugs_df[ (bugs_df['Type'] == 'enhancement') & (bugs_df['Type'] == 'enhancement') ].index\n",
    "bugs_df.drop(indexSevere , inplace=True)\n",
    "bugs_df.head(15)\n",
    "len(indexSevere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabf014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropped rows with Type \"Enhancement\" because they are not a bug but a new feature\n",
    "indexSevere = bugs_df[ (bugs_df['Type'] == 'task') & (bugs_df['Type'] == 'task') ].index\n",
    "bugs_df.drop(indexSevere , inplace=True)\n",
    "bugs_df.head(15)\n",
    "len(indexSevere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df= bugs_df[ (bugs_df['Severity'] == 'normal')]\n",
    "len(normal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc68dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_df= bugs_df[ (bugs_df['Severity'] == 'S3')]\n",
    "len(S3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Catagorise the severity level into a Severe and Non Severe to make it a binary problem\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"blocker\", \"Severity\"] = 'Severe'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"critical\", \"Severity\"] = 'Severe'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"major\", \"Severity\"] = 'Severe'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"S1\", \"Severity\"] = 'Severe'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"S2\", \"Severity\"] = 'Severe'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"S3\", \"Severity\"] = 'NonSevere'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"normal\", \"Severity\"] = 'NonSevere'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"minor\", \"Severity\"] = 'NonSevere'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"trivial\", \"Severity\"] = 'NonSevere'\n",
    "bugs_df.loc[bugs_df[\"Severity\"] == \"S4\", \"Severity\"] = 'NonSevere'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00af999",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df.Severity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b111fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bugs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df.Summary.str.contains(\"Browser\").sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c78f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540bc282",
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_df['Severity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fbca2",
   "metadata": {},
   "source": [
    "# Split data into test and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e197d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in rang(1,3):\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_data, testing_data = train_test_split(bugs_df, test_size=0.2, random_state=25)\n",
    "training_data, validation_data = train_test_split(training_data, test_size=0.2, random_state=25)\n",
    "\n",
    "print(f\"No. of training data: {training_data.shape[0]}\")\n",
    "print(f\"No. of validation data: {validation_data.shape[0]}\")\n",
    "print(f\"No. of testing data: {testing_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdataset = len(training_data)\n",
    "trainingdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingdataset = len(testing_data)\n",
    "testingdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df=training_data.reset_index()\n",
    "validation_data_df=validation_data.reset_index()\n",
    "testing_data_df=testing_data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed497faf",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad899db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenise the Summary text\n",
    "def nlpsteps(x):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(x))\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "    review = ' '.join(review)\n",
    "    return review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bcc1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_trainingdata = []\n",
    "for i in range(0,trainingdataset):\n",
    "    review = nlpsteps(str(training_data_df['Summary'][i]))\n",
    "    corpus_trainingdata.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_trainingdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus data splitted into separate words\n",
    "def convert(corpus_trainingdata):\n",
    "    return ([i for item in corpus_trainingdata for i in item.split()])\n",
    "     \n",
    "print( convert(corpus_trainingdata))\n",
    "splittedWords = convert(corpus_trainingdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7725ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of each words in the corpus\n",
    "import collections\n",
    "def getwordcounts(splittedWords):\n",
    "    occurrences = collections.Counter(splittedWords)\n",
    "    print(occurrences)\n",
    "    return occurrences\n",
    "\n",
    "splitted_words=getwordcounts(splittedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05848f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(splitted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02006634",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d477ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converted collection.counter into dictionary\n",
    "splitted_words_dict = dict(splitted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = splitted_words_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c48323",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that returns the counts for each words that falls in Severe or Non Severe category\n",
    "def get_distribution(val):\n",
    "    records = training_data_df[\n",
    "        training_data_df[\"Summary\"].str.contains(val)\n",
    "    ]\n",
    "    \n",
    "    if len(records) > 0:\n",
    "        res = training_data_df[\n",
    "            training_data_df[\"Summary\"].str.contains(val)\n",
    "        ][\"Severity\"].value_counts(dropna=False)\n",
    "        return dict(res)\n",
    "    return None\n",
    "    \n",
    "get_distribution(\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keys), len(list(set(keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}\n",
    "for key in keys:\n",
    "    res = get_distribution(key)\n",
    "    if res:\n",
    "        all_data[key] = res\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8948a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4869aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns Severe ratio\n",
    "def get_r1(ns,s):\n",
    "    return s/(s+ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_r1(203, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed7679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns NonSevere ratio\n",
    "def get_r2(ns,s):\n",
    "    return ns/(s+ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_r2(203,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8912a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_train = {}\n",
    "for key, value in all_data.items():\n",
    "    ns = value.get('NonSevere', 0)\n",
    "    s = value.get('Severe',0)\n",
    "    \n",
    "    r1 = get_r1(ns, s)\n",
    "    r2 = get_r2(ns, s)\n",
    "    \n",
    "    payload_train[key]= {\n",
    "        'r1': r1,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f48e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564b57e",
   "metadata": {},
   "source": [
    "## Test dictionaries with validation dataset on different thresholds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15bf9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incase of equal frequency the classifier will be Severe\n",
    "def classifier(Summary,severedictionary_list,nonseveredictionary_list):\n",
    "  \n",
    "    summaryList = Summary.split()\n",
    "    mytest_severe = len(set(severedictionary_list).intersection(summaryList))\n",
    "    mytest_nonsevere = len(set(nonseveredictionary_list).intersection(summaryList))\n",
    "    \n",
    "    if mytest_severe >= mytest_nonsevere:\n",
    "        tag = \"Severe\"\n",
    "    elif mytest_severe < mytest_nonsevere:\n",
    "        tag = \"NonSevere\"\n",
    "   \n",
    "    return tag\n",
    "   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# severe_threshold = [x * 0.01 for x in range(0, 100)] + [0.2, 0.5, 1.0]\n",
    "# severe_threshold=[x * 0.001 for x in range(0, 1000)] + [0.2, 0.5, 1.0]\n",
    "severe_threshold = [0.0, 0.1 ,0.2, 0.3 ,0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "nonsevere_threshold = [0.0, 0.1 ,0.2, 0.3 ,0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def functionname(x, y):\n",
    "# \"\"\"x is actually....\n",
    "#     y is actuallly ....\"\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates dictionary on different threholds and tests with validation data and returns the confusion matrix and accuracy scores of each dictionary\n",
    "def dictionary_onthresholds(severe_threshold, nonsevere_threshold, dataset):\n",
    "\n",
    "    severe_dictionary = {}\n",
    "    nonsevere_dictionary = {}\n",
    "    for keyy in payload_train:\n",
    "        if payload_train[keyy]['r1'] >= severe_threshold:\n",
    "            severe_dictionary[keyy] = payload_train[keyy]\n",
    "        \n",
    "        if payload_train[keyy]['r2'] >= nonsevere_threshold:\n",
    "            nonsevere_dictionary[keyy] = payload_train[keyy]\n",
    "            \n",
    "    severedictionary_list = list(severe_dictionary.keys())\n",
    "    print(severedictionary_list)\n",
    "    nonseveredictionary_list = list(nonsevere_dictionary.keys())\n",
    "    print(nonseveredictionary_list)\n",
    "    \n",
    "    dataset[\"Summary\"]  = dataset[\"Summary\"].apply(lambda x: nlpsteps(x))\n",
    "    \n",
    "    dataset[\"my_tag\"] = dataset[\"Summary\"].apply(lambda x: classifier(x,severedictionary_list,nonseveredictionary_list))\n",
    "    \n",
    "   \n",
    "    TP = 0 \n",
    "    for d in dataset.iterrows():\n",
    "        if ((d[1][\"my_tag\"]== \"Severe\") & (d[1][\"Severity\"]== \"Severe\")):\n",
    "            TP = TP+1\n",
    "    FP = 0 \n",
    "    for d in dataset.iterrows():\n",
    "        if (d[1][\"my_tag\"]== \"Severe\" )& (d[1][\"Severity\"]== \"NonSevere\"):\n",
    "            FP = FP+1\n",
    "    TN = 0 \n",
    "    for d in dataset.iterrows():\n",
    "        if (d[1][\"my_tag\"]== \"NonSevere\") & (d[1][\"Severity\"]== \"NonSevere\"):\n",
    "            TN = TN+1\n",
    "    FN = 0 \n",
    "    for d in dataset.iterrows():\n",
    "        if (d[1][\"my_tag\"]== \"NonSevere\") & (d[1][\"Severity\"]== \"Severe\"):\n",
    "            FN = FN+1\n",
    "\n",
    "    confusion_matrix = {\n",
    "        \"TP\": TP,\n",
    "        \"FP\": FP,\n",
    "        \"TN\": TN,\n",
    "        \"FN\": FN\n",
    "        \n",
    "    }\n",
    "    \n",
    "    confusion_matrix[\"Precision\"]= confusion_matrix[\"TP\"]/(confusion_matrix[\"TP\"]+confusion_matrix[\"FP\"]) if (confusion_matrix[\"TP\"]) !=0 else 0\n",
    "    confusion_matrix[\"Recall\"]= confusion_matrix[\"TP\"]/(confusion_matrix[\"TP\"]+confusion_matrix[\"FN\"]) if (confusion_matrix[\"TP\"]) !=0 else 0\n",
    "    confusion_matrix[\"F1Score\"] = 2*(confusion_matrix[\"Precision\"] * confusion_matrix[\"Recall\"])/(confusion_matrix[\"Precision\"] + confusion_matrix[\"Recall\"]) if (confusion_matrix[\"Precision\"] + confusion_matrix[\"Recall\"]) != 0 else 0\n",
    "                                                                                                   \n",
    "    \n",
    "    return confusion_matrix\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55655f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary_onthresholds(0.3,1.0,validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "possibleThesholdCombination = list(itertools.product(severe_threshold, nonsevere_threshold))\n",
    "result_list=[]\n",
    "for i in possibleThesholdCombination:\n",
    "   \n",
    "    severe_randomthreshold = i[0]\n",
    "    nonsevere_randomthreshold = i[1]\n",
    "    \n",
    "    count = dictionary_onthresholds(severe_randomthreshold,nonsevere_randomthreshold,validation_data)\n",
    "    result_dictionary = {\n",
    "     \"severe_threshold\": severe_randomthreshold,\n",
    "     \"nonsevere_threshold\":nonsevere_randomthreshold,\n",
    "#       \"performance\": count\n",
    "    \n",
    "    \n",
    "    }\n",
    "    result_dictionary.update(count)\n",
    "    result_list.append(result_dictionary)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    " result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1adac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1score of the dictionaries that was tested with  validation data\n",
    "F1Score_df = pd.DataFrame(result_list)\n",
    "F1Score_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fe025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max F1score that was tested with the validation data\n",
    "F1Score_df[F1Score_df['F1Score']==F1Score_df['F1Score'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930cada1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F1Score_df.sort_values(by=['F1Score'], ascending=False).head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3776c0b",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f85b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1score of the dictionaries that was tested with  testing data\n",
    "count = dictionary_onthresholds(0.2,0.9,testing_data)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ec093",
   "metadata": {},
   "source": [
    "# ML Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------ML Experiment Starts---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c622e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenised the training data again and stored into a different variable\n",
    "trainingdata_tokenised = []\n",
    "for i in range(0,trainingdataset):\n",
    "    review = nlpsteps(str(training_data_df['Summary'][i]))\n",
    "    trainingdata_tokenised.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenised the testing data\n",
    "testingdata_tokenised = []\n",
    "for i in range(0,testingdataset):\n",
    "    review = nlpsteps(str(testing_data_df['Summary'][i]))\n",
    "    testingdata_tokenised.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorised the Summary text in the training data and this for and transforms the \"tokenised Summary\" into an array\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "max_feature_list = []\n",
    "max_feature_accuracy = []\n",
    "SVM_list= []\n",
    "\n",
    "    \n",
    "features = [1000,10000,15000]\n",
    "\n",
    "for i in features:\n",
    "\n",
    "    cv = CountVectorizer(max_features = i)\n",
    "    X_train = cv.fit_transform(trainingdata_tokenised).toarray()\n",
    "    Y_train = training_data.iloc[:, -2].values\n",
    "    testingdata_vector = cv.transform(testingdata_tokenised)\n",
    "    X_test = testingdata_vector.toarray()\n",
    "    y_test = testing_data_df.iloc[:, -2].values\n",
    "    \n",
    "#------------------------------------SVM------------------------------------------------------------------\n",
    "    C_hyperparameter = [0.1,0.5,1,5,10,20,50,100]\n",
    "   \n",
    "    SVM_accuracy_list = []\n",
    "    \n",
    "    for c in C_hyperparameter:\n",
    "        SVM_dict = {}\n",
    "    \n",
    "        svm_model = SVC(C = c, kernel='linear', gamma='auto')\n",
    "        svm_model.fit(X_train,Y_train)\n",
    "\n",
    "        svm_pred = svm_model.predict(X_test)\n",
    "        svm_model = confusion_matrix(y_test, svm_pred)\n",
    "\n",
    "        SVM_accuracy_list= accuracy_score(y_test, svm_pred)\n",
    "        SVM_dict = {\"features\":i,\"C\":c, \"confusionmatrix\":svm_model,\"Accuracy\": SVM_accuracy_list }\n",
    "        SVM_list.append(SVM_dict)\n",
    "        SVM_list\n",
    "    \n",
    "    \n",
    " #-------------------------------------Naive Bayes-------------------------------------------------------------\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    \n",
    "    MultinomialNB_pred = classifier.predict(X_test)\n",
    "    \n",
    "    cm_MB = confusion_matrix(y_test, MultinomialNB_pred)\n",
    "    max_feature_accuracy = accuracy_score(y_test, MultinomialNB_pred)\n",
    "    \n",
    "    maxfeature_dict = {\"features\":i, \"confusionmatrix\": cm_MB ,\"Accuracy\": max_feature_accuracy }\n",
    "    max_feature_list.append(maxfeature_dict)\n",
    "    \n",
    "    #-------------------------------------Logistic Regression-------------------------------------------------------------\n",
    "    \n",
    "    lr_model = LogisticRegression()\n",
    "    lr_model.fit(X_train,Y_train)\n",
    "    \n",
    "    lr_pred = lr_model.predict(X_test)\n",
    "    \n",
    "  \n",
    "    cm_lr = confusion_matrix(y_test, lr_pred)\n",
    "    max_feature_accuracy =  accuracy_score(y_test, lr_pred)\n",
    "\n",
    "    maxfeature_dict = {\"features\":i, \"confusionmatrix\": cm_lr ,\"Accuracy\": max_feature_accuracy }\n",
    "    max_feature_list.append(maxfeature_dict)\n",
    "    \n",
    "    #------------------------------------Classification Report------------------------------------------------------------------\n",
    "    \n",
    "    print('Classification Reports\\n')\n",
    "    print(i)\n",
    "    print(f'MultinomialNB -------------------\\n{classification_report(y_test, MultinomialNB_pred)}\\n')\n",
    "    print(f'Logistic Regression-------------------\\n{classification_report(y_test, lr_pred)}\\n')\n",
    "    print(f'SVM -------------------\\n{classification_report(y_test, svm_pred)}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5989d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(max_feature_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303d0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e20319",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_df = pd.DataFrame(SVM_list)\n",
    "SVM_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578adccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment ENdss-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eeb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Reports\\n')\n",
    "print(f'MultinomialNB -------------------\\n{classification_report(y_test, MultinomialNB_pred)}\\n')\n",
    "print(f'Logistic Regression-------------------\\n{classification_report(y_test, lr_pred)}\\n')\n",
    "print(f'SVM -------------------\\n{classification_report(y_test, svm_pred)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aad0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy Classification\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, Y_train)\n",
    "DummyClassifier(strategy='most_frequent')\n",
    "dummy_pred = dummy_clf.predict(X_test)\n",
    "# array([1, 1, 1, 1])\n",
    "dummy_clf.score(X_test, y_test)\n",
    "print(f'Dummy Classifier -------------------\\n{classification_report(y_test, dummy_pred)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------FInalise code until here--- delete the below cells"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
